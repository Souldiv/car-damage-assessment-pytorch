{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"car_dam/data2a/\"\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "train_data = datasets.ImageFolder(data_dir + '/training', transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(data_dir + '/validation', transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet152(pretrained=True)\n",
    "fc_in_size = model.fc.in_features\n",
    "model.fc = nn.Sequential(OrderedDict([('fc1', nn.Linear(fc_in_size, 1024)),\n",
    "                                      ('dropout2', nn.Dropout(0.5)),\n",
    "                                      ('relu2', nn.ReLU()),\n",
    "                                      ('fc2', nn.Linear(1024, 3)),\n",
    "                                      ('output', nn.LogSoftmax(dim=1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2048])\n",
      "torch.Size([1024])\n",
      "torch.Size([3, 1024])\n",
      "torch.Size([3])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if n.split('.')[0] == 'fc':\n",
    "        pass\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "count = 0\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p.size())\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, testloader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in testloader:\n",
    "\n",
    "        output = model.forward(images.cuda())\n",
    "        labels = labels.type(torch.LongTensor).cuda()\n",
    "        test_loss += criterion(output, labels).item()\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_run = True\n",
    "inf = torch.load('Models/3_Classify/resnet152_epoch34_step280.pkl')\n",
    "model.load_state_dict(inf['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100..  Training Loss: 0.7381..  valid Loss: 0.7688.. \n",
      "Epoch: 3/100..  Training Loss: 0.7253..  valid Loss: 0.8072.. \n",
      "Epoch: 4/100..  Training Loss: 0.7495..  valid Loss: 0.7495.. \n",
      "Epoch: 5/100..  Training Loss: 0.7240..  valid Loss: 0.8005.. \n",
      "Epoch: 7/100..  Training Loss: 0.7416..  valid Loss: 0.7601.. \n",
      "Epoch: 8/100..  Training Loss: 0.7310..  valid Loss: 0.7608.. \n",
      "Epoch: 9/100..  Training Loss: 0.6997..  valid Loss: 0.7839.. \n",
      "Epoch: 10/100..  Training Loss: 0.7282..  valid Loss: 0.7591.. \n",
      "Epoch: 12/100..  Training Loss: 0.7467..  valid Loss: 0.7674.. \n",
      "Epoch: 13/100..  Training Loss: 0.7095..  valid Loss: 0.7966.. \n",
      "Epoch: 14/100..  Training Loss: 0.7383..  valid Loss: 0.7748.. \n",
      "Epoch: 15/100..  Training Loss: 0.7258..  valid Loss: 0.7682.. \n",
      "Epoch: 17/100..  Training Loss: 0.6895..  valid Loss: 0.7810.. \n",
      "Epoch: 18/100..  Training Loss: 0.7366..  valid Loss: 0.7688.. \n",
      "Epoch: 19/100..  Training Loss: 0.7121..  valid Loss: 0.7800.. \n",
      "Epoch: 20/100..  Training Loss: 0.7169..  valid Loss: 0.7572.. \n",
      "Epoch: 22/100..  Training Loss: 0.7288..  valid Loss: 0.7630.. \n",
      "Epoch: 23/100..  Training Loss: 0.7076..  valid Loss: 0.7717.. \n",
      "Epoch: 24/100..  Training Loss: 0.6974..  valid Loss: 0.8056.. \n",
      "Epoch: 25/100..  Training Loss: 0.7060..  valid Loss: 0.7404.. \n",
      "Epoch: 27/100..  Training Loss: 0.7433..  valid Loss: 0.7649.. \n",
      "Epoch: 28/100..  Training Loss: 0.7438..  valid Loss: 0.7748.. \n",
      "Epoch: 29/100..  Training Loss: 0.7315..  valid Loss: 0.7720.. \n",
      "Epoch: 30/100..  Training Loss: 0.7147..  valid Loss: 0.7608.. \n",
      "Epoch: 32/100..  Training Loss: 0.7295..  valid Loss: 0.7428.. \n",
      "Epoch: 33/100..  Training Loss: 0.7347..  valid Loss: 0.7788.. \n",
      "Epoch: 34/100..  Training Loss: 0.7460..  valid Loss: 0.7453.. \n",
      "Epoch: 35/100..  Training Loss: 0.7144..  valid Loss: 0.7727.. \n",
      "Epoch: 37/100..  Training Loss: 0.7318..  valid Loss: 0.7603.. \n",
      "Epoch: 38/100..  Training Loss: 0.7344..  valid Loss: 0.7781.. \n",
      "Epoch: 39/100..  Training Loss: 0.7288..  valid Loss: 0.7353.. \n",
      "Epoch: 40/100..  Training Loss: 0.7127..  valid Loss: 0.7972.. \n",
      "Epoch: 42/100..  Training Loss: 0.6867..  valid Loss: 0.7503.. \n",
      "Epoch: 43/100..  Training Loss: 0.6779..  valid Loss: 0.7492.. \n",
      "Epoch: 44/100..  Training Loss: 0.7744..  valid Loss: 0.7672.. \n",
      "Epoch: 45/100..  Training Loss: 0.7148..  valid Loss: 0.7667.. \n",
      "Epoch: 47/100..  Training Loss: 0.6966..  valid Loss: 0.7763.. \n",
      "Epoch: 48/100..  Training Loss: 0.7121..  valid Loss: 0.7640.. \n",
      "Epoch: 49/100..  Training Loss: 0.7352..  valid Loss: 0.7794.. \n",
      "Epoch: 50/100..  Training Loss: 0.7087..  valid Loss: 0.7685.. \n",
      "Epoch: 52/100..  Training Loss: 0.6810..  valid Loss: 0.7416.. \n",
      "Epoch: 53/100..  Training Loss: 0.7125..  valid Loss: 0.7663.. \n",
      "Epoch: 54/100..  Training Loss: 0.7213..  valid Loss: 0.7537.. \n",
      "Epoch: 55/100..  Training Loss: 0.7087..  valid Loss: 0.7405.. \n",
      "Epoch: 57/100..  Training Loss: 0.7166..  valid Loss: 0.7702.. \n",
      "Epoch: 58/100..  Training Loss: 0.7231..  valid Loss: 0.7655.. \n",
      "Epoch: 59/100..  Training Loss: 0.7092..  valid Loss: 0.7572.. \n",
      "Epoch: 60/100..  Training Loss: 0.7137..  valid Loss: 0.7499.. \n",
      "Epoch: 62/100..  Training Loss: 0.7077..  valid Loss: 0.7678.. \n",
      "Epoch: 63/100..  Training Loss: 0.6873..  valid Loss: 0.7474.. \n",
      "Epoch: 64/100..  Training Loss: 0.7158..  valid Loss: 0.7618.. \n",
      "Epoch: 65/100..  Training Loss: 0.7187..  valid Loss: 0.7661.. \n",
      "Epoch: 67/100..  Training Loss: 0.6727..  valid Loss: 0.7727.. \n",
      "Epoch: 68/100..  Training Loss: 0.6917..  valid Loss: 0.8113.. \n",
      "Epoch: 69/100..  Training Loss: 0.7260..  valid Loss: 0.7505.. \n",
      "Epoch: 70/100..  Training Loss: 0.7135..  valid Loss: 0.7591.. \n",
      "Epoch: 72/100..  Training Loss: 0.6769..  valid Loss: 0.7459.. \n",
      "Epoch: 73/100..  Training Loss: 0.7457..  valid Loss: 0.7519.. \n",
      "Epoch: 74/100..  Training Loss: 0.7238..  valid Loss: 0.7698.. \n",
      "Epoch: 75/100..  Training Loss: 0.6956..  valid Loss: 0.7713.. \n",
      "Epoch: 77/100..  Training Loss: 0.6954..  valid Loss: 0.7685.. \n",
      "Epoch: 78/100..  Training Loss: 0.7093..  valid Loss: 0.7844.. \n",
      "Epoch: 79/100..  Training Loss: 0.7118..  valid Loss: 0.7790.. \n",
      "Epoch: 80/100..  Training Loss: 0.7108..  valid Loss: 0.7595.. \n",
      "Epoch: 82/100..  Training Loss: 0.7051..  valid Loss: 0.7732.. \n",
      "Epoch: 83/100..  Training Loss: 0.7102..  valid Loss: 0.7547.. \n",
      "Epoch: 84/100..  Training Loss: 0.7236..  valid Loss: 0.7361.. \n",
      "Epoch: 85/100..  Training Loss: 0.6924..  valid Loss: 0.7566.. \n",
      "Epoch: 87/100..  Training Loss: 0.6965..  valid Loss: 0.8032.. \n",
      "Epoch: 88/100..  Training Loss: 0.7378..  valid Loss: 0.7403.. \n",
      "Epoch: 89/100..  Training Loss: 0.6992..  valid Loss: 0.7553.. \n",
      "Epoch: 90/100..  Training Loss: 0.6738..  valid Loss: 0.7482.. \n",
      "Epoch: 92/100..  Training Loss: 0.7099..  valid Loss: 0.7556.. \n",
      "Epoch: 93/100..  Training Loss: 0.6901..  valid Loss: 0.7411.. \n",
      "Epoch: 94/100..  Training Loss: 0.7195..  valid Loss: 0.7579.. \n",
      "Epoch: 95/100..  Training Loss: 0.6907..  valid Loss: 0.7270.. \n",
      "Epoch: 97/100..  Training Loss: 0.6841..  valid Loss: 0.7867.. \n",
      "Epoch: 98/100..  Training Loss: 0.6994..  valid Loss: 0.7924.. \n",
      "Epoch: 99/100..  Training Loss: 0.7179..  valid Loss: 0.7330.. \n",
      "Epoch: 100/100..  Training Loss: 0.6985..  valid Loss: 0.7655.. \n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = \"Models/3_Classify/\"\n",
    "\n",
    "# writer = SummaryWriter()\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-4)\n",
    "if checkpoint_run:\n",
    "    optimizer.load_state_dict(inf['optimizer_state_dict'])\n",
    "\n",
    "best_train_loss = 0.7\n",
    "best_val_loss = 0.7\n",
    "\n",
    "# scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[5, 15, 25, 40, 50], gamma=0.1)\n",
    "\n",
    "num_epochs = 100\n",
    "running_loss = 0\n",
    "steps = 0\n",
    "print_every = 10\n",
    "log_every = 10\n",
    "log_step = 0\n",
    "\n",
    "\n",
    "log_every = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "#     scheduler.step()\n",
    "    for data_ in trainloader:\n",
    "        steps += 1\n",
    "        img, bbox = data_\n",
    "        \n",
    "        img = img.cuda()\n",
    "        target = bbox.type(torch.LongTensor).cuda()\n",
    "        \n",
    "        output = model(img)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % log_every == 0:\n",
    "            log_step += 1\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_loss = validation(model, testloader, criterion)\n",
    "            model.train()\n",
    "            \n",
    "            train_loss = running_loss/log_every\n",
    "            val_loss = valid_loss/len(testloader)\n",
    "            \n",
    "#             writer.add_scalar('Training Loss', train_loss, log_step)\n",
    "#             writer.add_scalar('Validation Loss', val_loss, log_step)\n",
    "#             writer.add_scalar('Learning rate', optimizer.state_dict()['param_groups'][0]['lr'], log_step)\n",
    "            \n",
    "            if val_loss < best_val_loss and train_loss < best_train_loss:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss\n",
    "            }, SAVE_PATH + \"resnet152_3_epoch{}_step{}.pkl\".format(epoch, steps))\n",
    "                best_train_loss = train_loss\n",
    "                best_val_loss = val_loss\n",
    "            running_loss = 0\n",
    "            \n",
    "        if steps % print_every == 0:\n",
    "            print(\"Epoch: {}/{}.. \".format(epoch+1, num_epochs),\n",
    "                  \"Training Loss: {:.4f}.. \".format(train_loss),\n",
    "                  \"valid Loss: {:.4f}.. \".format(val_loss))\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "y_pred = list()\n",
    "y_true = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, op in iter(testloader):\n",
    "        k = model.forward(imgs.cuda())\n",
    "        y_pred.extend(list(np.argmax(k.cpu().data.numpy(), axis=1)))\n",
    "        y_true.extend(op.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD8CAYAAAA2Y2wxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFbdJREFUeJzt3XmYVPWVxvHv6aYRWaTZ9wFjiBKN\naDSI4mgEg0s04igmrkSZdKJoMO6akMQxRo0LweVJRFAgQQSjiFFcEUeICqKoKMugqIg0Oy07UlVn\n/ugKNgJd1VK/utW330+e+3TXrapbx3583hzP/dW95u6IiEg4RVEXICISdwpaEZHAFLQiIoEpaEVE\nAlPQiogEpqAVEQlMQSsiEpiCVkQkMAWtiEhg9UJ/wLZVi/TVs8C+3a1/1CXEXte920RdQp0wefFk\n29Nj1CRzSlp+Y48/LxvqaEVEAgve0YqI5FUqGXUFO1HQiki8JBNRV7ATBa2IxIp7KuoSdqKgFZF4\nSSloRUTCUkcrIhKYToaJiASmjlZEJCzXqgMRkcB0MkxEJDCNDkREAtPJMBGRwHLY0ZrZx8B6IAkk\n3P1wM2sOjAe6AB8DZ7n72uqOo4vKiEi8JBPZb9k5zt0PcffD04+vA6a4e1dgSvpxtRS0IhIvqVT2\n29dzGjA6/ftooF+mNyhoRSRW3JNZb9kcDnjezN40s7L0vjbuXl75WV4OtM50EM1oRSReajCjTYdn\nWZVdw919eJXHvdx9qZm1Bl4ws/lfpyQFrYjESw1GAulQHV7N80vTP1eY2USgB7DczNq5e7mZtQNW\nZPocjQ5EJF48lf1WDTNrZGZN/v070Bd4D3gSGJB+2QBgUqaS1NGKSLwkt+XqSG2AiWYGlVn5sLs/\na2ZvABPMbCCwGMh40z4FrYjES46+guvui4Duu9i/GuhTk2MpaEUkXvQVXBGRwHRRGRGRwBS0IiJh\nee5OhuWMglZE4kUzWhGRwDQ6EBEJTB2tiEhg6mhFRAJTRysiElhCd8EtOH3PGECjhg0pKiqiuLiY\nCQ/evf25hx7+B3feN5JpTz9Cs9KmEVYZH/vu15lhI27Z/rhT5w4Mu+2vjLp/XIRV1X6X3345Pfr0\noGJ1BZf84BIALrrhIo44/ggS2xKUf1LO0KuGsnHdxogrzQN1tIXpwXtu3SlIy5ev5LU3ZtOuTcZr\n+koNfPThJ/zouHMAKCoqYvqcZ3j+6akRV1X7vfjoi/xz9D+5cuiV2/fNnjabUbeNIpVMceH1F3LW\noLN46JaHIqwyTwpwRpvxMolmdoCZXWtmd5vZsPTv3fJRXJT+dPf9XHHJQCov3CMhHHVMDxZ/vISl\nS5ZFXUqt997M91hfsX6HfbOnzSaVrAyd+W/Np2XbllGUln85ukxiLlUbtGZ2LfAIYMBM4I307+PM\nLOMNyWoDM6PsV7/mrIsu49FJkwGYOu11WrdqyQFdvxFxdfH2w9P78tTjz0VdRp3Q98d9mfXyrKjL\nyI/w9wyrsUyjg4HAge6+w3fazOwu4H3g1lCF5cvf/nInrVu1YPXaCn52+Q3s27kTw8c8wvChN0dd\nWqyVlNSj9wnHcscf7o26lNj78aU/JplIMnViHRnRFOCMNtPoIAW038X+dunndsnMysxslpnNGjGm\nsE9ytG7VAoAWzUrpc8xRzJo9h8+WLuOMAZfQ94wBLF+5iv4XXcaq1WsirjRejunTi7nvzmf1Sv1d\nQ+pzZh969OnB7b+8PepS8ieRyH7Lk0wd7eXAFDNbCHya3vcfwDeBS3f3pqr34dm2apHnoM4gNm3e\ngqdSNGrUkE2bt/DqzLe4+MJzeOXpR7a/pu8ZAxg/8m6tOsixU/7rBJ6a+GzUZcTaYcceRv+L+3NN\n/2vYumVr1OXkjxde5FQbtOnbNnyLyhuSdaByPrsEeMOzvFdvIVu9Zi2Db7gJgGQiycl9v8/RPQ+P\nuKr4a7B3A3odewRDrvxj1KXExjX3XMPBRx7MPs32YcyMMfz9rr9z1qCzKKlfws1jK8dgC2Yv4N4b\n6sCopgBXHZgHTv9C7mjj4tvdMt6ySPZQ173bRF1CnTB58eQ9XuezeeyQrDNn73Nvysu6Iq2jFZF4\nKcCTYQpaEYmXZOFNNRW0IhIvBTijVdCKSLwoaEVEAtOMVkQkLE8V3kInBa2IxItGByIigWnVgYhI\nYOpoRUQCU9CKiARW2y4qIyJS66ijFREJTMu7REQCK8BVBxlvzigiUpt4KpX1lg0zKzaz2Wb2VPrx\nvmY2w8wWmtl4M6uf6RgKWhGJl5Rnv2VnMDCvyuPbgKHu3hVYS+W9FauloBWReMnh7cbNrCPwQ2BE\n+rEBvYF/pF8yGuiX6Tia0YpIvOT2ZNifgWuAJunHLYAKd//3nR2XUHmbr2qpoxWReEkks96q3rE7\nvZX9+zBmdgqwwt3frHL0Xd36JmOyq6MVkXipwWUSq96xexd6AT8ys5OBBsA+VHa4pWZWL93VdgSW\nZvocdbQiEi85Ohnm7te7e0d37wL8BHjJ3c8FpgJnpl82AJiUqSQFrYjESq6Xd+3CtcAVZvYBlTPb\nkZneoNGBiMRLgG+GufvLwMvp3xcBPWryfgWtiMSLvoIrIhJYAX4FV0ErIrGie4aJiISmoBURCUzX\noxURCUwdrYhIYApaEZGwPFkHRwcDDrsy9EfUeaOK9426hNgbvG151CVIttTRioiEpeVdIiKhKWhF\nRAIrvBGtglZE4sUThZe0CloRiZfCy1kFrYjEi06GiYiEpo5WRCQsdbQiIqGpoxURCcsTUVewMwWt\niMRKDe42njcKWhGJFwWtiEhY6mhFRAJT0IqIBOZJi7qEnShoRSRW1NGKiATmKXW0IiJBqaMVEQnM\nXR2tiEhQ6mhFRAJLadWBiEhYOhkmIhKYglZEJDAvvMvRUhR1ASIiueQpy3qrjpk1MLOZZvaOmb1v\nZjem9+9rZjPMbKGZjTez+plqUtCKSKy4W9ZbBluB3u7eHTgEONHMegK3AUPdvSuwFhiY6UAKWhGJ\nlWTSst6q45U2pB+WpDcHegP/SO8fDfTLVJOCVkRiJYcdLWZWbGZvAyuAF4APgQr37fdxWAJ0yHQc\nBa2IxEpNZrRmVmZms6psZTscyz3p7ocAHYEeQLddfWSmmrTqQERipSarDtx9ODA8i9dVmNnLQE+g\n1MzqpbvajsDSTO9XRysisZLDVQetzKw0/fvewPHAPGAqcGb6ZQOASZlqqvMdbdntl3Jo78NZt/pz\nru07eIfnflh2Guf++qf8/JALWL92fUQV1m7127fgW/dcRkmrUnBn2d9eoHzEZPa//1c02K89APWa\nNiLx+UbeOf7qiKuNj5/895mcfu6pYMYTY//JuAcejbqkvEmmctY/tgNGm1kxlU3pBHd/yszmAo+Y\n2R+A2cDITAeq80H7yqMv8fzoyVx8144h27xdC75zdHdWLlkRUWXx4IkkH/1+NBvnfERxowZ0f/5P\nVLzyLgt+PnT7a7r8/gKS6zZFWGW87Lf/vpx+7qlccHIZiS8S3P3wHUx/8TU+/WhJ1KXlRa6+sODu\n7wKH7mL/IirntVmr86OD+TPnsqFi5271/N9exMO3jMlizC3V2baigo1zPgIguXELmxZ+Rv22zXd4\nTctTj2LlxOlRlBdLXbp2Zs6bc9m6eSvJZJK3Xn+b4046Juqy8ibllvWWL3U+aHflu8d/j7XL1rB4\n3sdRlxIre3VqReODurDhrYXb9+3TsxvbVn3Olo+WRVhZvHy44CMO7dmdps32Ya+996JX7560ad86\n6rLyJpfLu3Lla48OzOxCd38ol8UUgvoN6tPv0jO55fwboy4lVooaNuCAEVex6LejSG7YvH1/y9OP\nVjebYx8v/IQx943lvvFD2bRxEwvnfkAymYy6rLyJ27UOdptEVdemfbDh4z34iPxr07ktrTq14dZn\nhjJs+v00b9eCm5++k6atSqMurdayesUcMPIqVj4+jTWTZ3z5RHERLU4+glWT/hVdcTE1adzTnNd3\nIGWnX8bnFetZvOjTqEvKm0IcHVTb0ZrZu7t7Cmizu/dVXZt2TufTC/D/X3bv0wWLufiwn25/PGz6\n/fzm1Ku06mAPfHPoJWxeuISl9z+1w/7SYw5m8wef8UX5mogqi69mLUpZu7qCNh1a0/vkY7jwlF9E\nXVLe5HDVQc5kGh20AU6g8sIJVRnwapCK8uzSu6+g25EH0qTZPtzz+gM8NvQRXh4/JeqyYqNJjwNo\n3f9YNs79hO4v3g7A4lseZu2U2bTs14tVE9XNhvCnkX+gabOmJLYluO36oaz/fEPmN8VEIXZ25tUM\nNMxsJPCQu+80RDOzh939nEwfUNs62tpo0NbiqEuIvcG2POoS6oRZ5dP2+L/nX213RtaZc1T5Y3mZ\nH1Tb0br7bi//lU3Iiojkm+6CKyISWAHeBFdBKyLx4qijFREJKqHRgYhIWOpoRUQC04xWRCQwdbQi\nIoGpoxURCSypjlZEJKwMd6iJhIJWRGIlpY5WRCSsQry4ioJWRGJFJ8NERAJLmUYHIiJBFeJNexS0\nIhIrWnUgIhKYVh2IiASmVQciIoFpdCAiEpiWd4mIBJZURysiEpY6WhGRwBS0IiKBFeAtwxS0IhIv\n6mhFRAIrxK/gFkVdgIhILqUs+606ZtbJzKaa2Twze9/MBqf3NzezF8xsYfpns0w1KWhFJFZSNdgy\nSABXuns3oCcwyMy+DVwHTHH3rsCU9ONqKWhFJFZyFbTuXu7ub6V/Xw/MAzoApwGj0y8bDfTLVJNm\ntCISKyGudWBmXYBDgRlAG3cvh8owNrPWmd6vjlZEYqUmM1ozKzOzWVW2sq8ez8waA48Bl7v7uq9T\nkzpaEYmVmqw6cPfhwPDdPW9mJVSG7Fh3fzy9e7mZtUt3s+2AFZk+J3jQfpL4PPRH1HnHrlkQdQmx\nt6r//lGXIFlK5Wh4YGYGjATmuftdVZ56EhgA3Jr+OSnTsdTRikis5PALC72A84E5ZvZ2et8NVAbs\nBDMbCCwG+mc6kIJWRGIlVyfD3H067PZ2DX1qciwFrYjEir6CKyISWMIK72Y2CloRiZXCi1kFrYjE\njEYHIiKB5Wp5Vy4paEUkVgovZhW0IhIzGh2IiASWLMCeVkErIrGijlZEJDBXRysiEpY6WhGRwLS8\nS0QksMKLWQWtiMRMogCjVkErIrGik2EiIoHpZJiISGDqaEVEAlNHKyISWNLV0YqIBKV1tCIigWlG\nKyISmGa0IiKBaXQgIhKYRgciIoFp1YGISGAaHYiIBKaTYSIigWlGKyISmEYHBej6O6+m1/E9Wbuq\ngvP7DASgSWkTbvrLENp2asuyT5cx5Bf/w/rPN0RcaTx07NieUQ8Oo03bVqRSKUaMGMs9946Muqza\nr6SExkOGYfVKoLiYbTP/ly2PjQagQf+LKDniWEil2DrlSb54bmLExYblOhlWeCZPeI7HHnqCIcOu\n277v/EFnM2v6bP5+3zjOG3Q25w06m7/88YEIq4yPRCLB1dfcyOy336Nx40bMnPEsL055hXnzFkZd\nWu22bRsbbr4Ctm6B4mIa//Zuit+ZSXH7zhS1aM36q38K7tg+pVFXGlwh3m68KNMLzOwAM+tjZo2/\nsv/EcGXlzzsz3mVdxbod9v3nCb145tHnAHjm0ec45sSjoygtlpYtW8Hst98DYMOGjcyfv5AO7dtG\nXFVMbN1S+bO4XuXmTv3jf8SWiWMg3eX5uooIC8yPFJ71li/VdrRm9ktgEDAPGGlmg919UvrpPwLP\nBq4vEs1aNmP1ijUArF6xhtIW8e8CotC5c0cO6X4QM2bOjrqUeLAimtz8V4radGDrC0+Q/HA+Ra3b\nUdLzOEoOPxpfX8Hm0feSWv5Z1JUGVYijg0wd7c+Aw9y9H/B9YIiZDU4/ZyELk3hr1KghE8Y/wBVX\n/Y716zX/zglPsf6GMtZddhbF+x1AUccuWEl92PYFG4ZczBcvTaZh2dVRVxlcLjtaM3vQzFaY2XtV\n9jU3sxfMbGH6Z7NMx8kUtMXuvgHA3T+mMmxPMrO7qCZozazMzGaZ2axlG5dm/IcpNGtXraVF6+YA\ntGjdnIrV8f/PrXyqV68ej45/gHHjJvLEE89EXU7s+KaNJOa9Q8nBPUitWcm2ma8AsG3WNIr/4xsR\nVxee1+B/WRgFfHVMeh0wxd27AlPSj6uVKWiXmdkh2/8BKkP3FKAl8J3dvcndh7v74e5+eNtG7TPV\nUHCmP/8qJ/U/AYCT+p/AtOf+FXFF8fLA8DuZN/8D/jxseNSlxIY1aYo1bFT5oKQ+JQd+l2T5YrbN\n+hf1DjwUgHrdupMsXxJhlfmRdM96y8TdXwHWfGX3acDo9O+jgX6ZjpNp1cEFQOIrH5wALjCz+zNW\nWQv8/r7fcOiR3Slt3pSJs8Yz8o5R/O2+cdz0199yytknsfyzFfzm5zdGXWZs9Drqe5x/3pm8O2cu\ns954HoAhQ27lmWdfiriy2s1KW9DwF9diRUVgRXwx42USs18nuWAODS/5NXuddCa+ZTObRtwRdanB\n1eQkl5mVAWVVdg1390wdQBt3Lwdw93Iza53xc0IPjnt16F14k+mYmbFyQdQlxN6q/vtHXUKdUDr2\npT0+93Nkh+OyzpzXPpua8fPMrAvwlLsflH5c4e6lVZ5f6+7Vzmnr/DpaEYmXPKw6WG5m7dLdbDtg\nRaY3ZFxHKyJSm+RhHe2TwID07wOASdW8FlBHKyIxk8uLypjZOCpXW7U0syXA74BbgQlmNhBYDPTP\ndBwFrYjEStJzd6FEdz97N0/1qclxFLQiEiuF+M0wBa2IxIoukygiEpgu/C0iElhKowMRkbDU0YqI\nBJbLVQe5oqAVkVjR6EBEJDCNDkREAlNHKyISmDpaEZHAkp6MuoSdKGhFJFb0FVwRkcD0FVwRkcDU\n0YqIBKZVByIigWnVgYhIYPoKrohIYJrRiogEphmtiEhg6mhFRALTOloRkcDU0YqIBKZVByIigelk\nmIhIYBodiIgEpm+GiYgEpo5WRCSwQpzRWiGmf9TMrMzdh0ddR5zpbxye/saFoyjqAgpUWdQF1AH6\nG4env3GBUNCKiASmoBURCUxBu2uaa4Wnv3F4+hsXCJ0MExEJTB2tiEhgCtoqzOxEM1tgZh+Y2XVR\n1xNHZvagma0ws/eiriWuzKyTmU01s3lm9r6ZDY66prpOo4M0MysG/g/4AbAEeAM4293nRlpYzJjZ\nMcAGYIy7HxR1PXFkZu2Adu7+lpk1Ad4E+unf5eioo/1SD+ADd1/k7l8AjwCnRVxT7Lj7K8CaqOuI\nM3cvd/e30r+vB+YBHaKtqm5T0H6pA/BplcdL0L+cUsuZWRfgUGBGtJXUbQraL9ku9mmuIrWWmTUG\nHgMud/d1UddTlylov7QE6FTlcUdgaUS1iOwRMyuhMmTHuvvjUddT1ylov/QG0NXM9jWz+sBPgCcj\nrkmkxszMgJHAPHe/K+p6REG7nbsngEuB56g8eTDB3d+Ptqr4MbNxwGvA/ma2xMwGRl1TDPUCzgd6\nm9nb6e3kqIuqy7S8S0QkMHW0IiKBKWhFRAJT0IqIBKagFREJTEErIhKYglZEJDAFrYhIYApaEZHA\n/h/it15F/sBwCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf6f53cb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_pred=y_pred, y_true=y_true)\n",
    "_ = sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.72        73\n",
      "           1       0.75      0.54      0.63        50\n",
      "           2       0.63      0.75      0.69        48\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       171\n",
      "   macro avg       0.69      0.68      0.68       171\n",
      "weighted avg       0.69      0.68      0.68       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_true, y_pred=y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
