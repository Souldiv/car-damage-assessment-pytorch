{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"car_dam/data2a/\"\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "train_data = datasets.ImageFolder(data_dir + '/training', transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(data_dir + '/validation', transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet152(pretrained=True)\n",
    "fc_in_size = model.fc.in_features\n",
    "model.fc = nn.Sequential(OrderedDict([('fc1', nn.Linear(fc_in_size, 1024)),\n",
    "                                      ('dropout2', nn.Dropout(0.5)),\n",
    "                                      ('relu2', nn.ReLU()),\n",
    "                                      ('fc2', nn.Linear(1024, 3)),\n",
    "                                      ('output', nn.LogSoftmax(dim=1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2048])\n",
      "torch.Size([1024])\n",
      "torch.Size([3, 1024])\n",
      "torch.Size([3])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if n.split('.')[0] == 'fc':\n",
    "        pass\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "count = 0\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p.size())\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, testloader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in testloader:\n",
    "\n",
    "        output = model.forward(images.cuda())\n",
    "        labels = labels.type(torch.LongTensor).cuda()\n",
    "        test_loss += criterion(output, labels).item()\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_run = True\n",
    "inf = torch.load('Models/3_Classify/resnet152_epoch34_step280.pkl')\n",
    "model.load_state_dict(inf['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100..  Training Loss: 1.0421..  valid Loss: 1.0321.. \n",
      "Epoch: 3/100..  Training Loss: 0.9712..  valid Loss: 0.9588.. \n",
      "Epoch: 4/100..  Training Loss: 0.9016..  valid Loss: 0.9276.. \n",
      "Epoch: 5/100..  Training Loss: 0.8626..  valid Loss: 0.8864.. \n",
      "Epoch: 7/100..  Training Loss: 0.8442..  valid Loss: 0.7695.. \n",
      "Epoch: 8/100..  Training Loss: 0.8273..  valid Loss: 0.7722.. \n",
      "Epoch: 9/100..  Training Loss: 0.7759..  valid Loss: 0.8208.. \n",
      "Epoch: 10/100..  Training Loss: 0.7585..  valid Loss: 0.7959.. \n",
      "Epoch: 12/100..  Training Loss: 0.7529..  valid Loss: 0.7643.. \n",
      "Epoch: 13/100..  Training Loss: 0.7332..  valid Loss: 0.7541.. \n",
      "Epoch: 14/100..  Training Loss: 0.7185..  valid Loss: 0.8207.. \n",
      "Epoch: 15/100..  Training Loss: 0.7098..  valid Loss: 0.7888.. \n",
      "Epoch: 17/100..  Training Loss: 0.7134..  valid Loss: 0.7887.. \n",
      "Epoch: 18/100..  Training Loss: 0.6694..  valid Loss: 0.7405.. \n",
      "Epoch: 19/100..  Training Loss: 0.7147..  valid Loss: 0.7157.. \n",
      "Epoch: 20/100..  Training Loss: 0.6958..  valid Loss: 0.7394.. \n",
      "Epoch: 22/100..  Training Loss: 0.6651..  valid Loss: 0.7975.. \n",
      "Epoch: 23/100..  Training Loss: 0.6728..  valid Loss: 0.7710.. \n",
      "Epoch: 24/100..  Training Loss: 0.6596..  valid Loss: 0.7673.. \n",
      "Epoch: 25/100..  Training Loss: 0.7003..  valid Loss: 0.7713.. \n",
      "Epoch: 27/100..  Training Loss: 0.6775..  valid Loss: 0.7613.. \n",
      "Epoch: 28/100..  Training Loss: 0.6439..  valid Loss: 0.7841.. \n",
      "Epoch: 29/100..  Training Loss: 0.6525..  valid Loss: 0.7931.. \n",
      "Epoch: 30/100..  Training Loss: 0.6768..  valid Loss: 0.8099.. \n",
      "Epoch: 32/100..  Training Loss: 0.6183..  valid Loss: 0.7365.. \n",
      "Epoch: 33/100..  Training Loss: 0.6328..  valid Loss: 0.7375.. \n",
      "Epoch: 34/100..  Training Loss: 0.6450..  valid Loss: 0.7405.. \n",
      "Epoch: 35/100..  Training Loss: 0.6229..  valid Loss: 0.7211.. \n",
      "Epoch: 37/100..  Training Loss: 0.6213..  valid Loss: 0.7472.. \n",
      "Epoch: 38/100..  Training Loss: 0.6071..  valid Loss: 0.7158.. \n",
      "Epoch: 39/100..  Training Loss: 0.6250..  valid Loss: 0.7600.. \n",
      "Epoch: 40/100..  Training Loss: 0.6242..  valid Loss: 0.7411.. \n",
      "Epoch: 42/100..  Training Loss: 0.6333..  valid Loss: 0.7625.. \n",
      "Epoch: 43/100..  Training Loss: 0.5969..  valid Loss: 0.8172.. \n",
      "Epoch: 44/100..  Training Loss: 0.6214..  valid Loss: 0.7805.. \n",
      "Epoch: 45/100..  Training Loss: 0.5952..  valid Loss: 0.7916.. \n",
      "Epoch: 47/100..  Training Loss: 0.6142..  valid Loss: 0.7483.. \n",
      "Epoch: 48/100..  Training Loss: 0.5867..  valid Loss: 0.8044.. \n",
      "Epoch: 49/100..  Training Loss: 0.6127..  valid Loss: 0.8163.. \n",
      "Epoch: 50/100..  Training Loss: 0.5807..  valid Loss: 0.7190.. \n",
      "Epoch: 52/100..  Training Loss: 0.6088..  valid Loss: 0.7949.. \n",
      "Epoch: 53/100..  Training Loss: 0.6011..  valid Loss: 0.8126.. \n",
      "Epoch: 54/100..  Training Loss: 0.5918..  valid Loss: 0.7822.. \n",
      "Epoch: 55/100..  Training Loss: 0.5628..  valid Loss: 0.7087.. \n",
      "Epoch: 57/100..  Training Loss: 0.5988..  valid Loss: 0.6896.. \n",
      "Epoch: 58/100..  Training Loss: 0.5746..  valid Loss: 0.7672.. \n",
      "Epoch: 59/100..  Training Loss: 0.5486..  valid Loss: 0.7852.. \n",
      "Epoch: 60/100..  Training Loss: 0.5887..  valid Loss: 0.7124.. \n",
      "Epoch: 62/100..  Training Loss: 0.5697..  valid Loss: 0.7087.. \n",
      "Epoch: 63/100..  Training Loss: 0.5694..  valid Loss: 0.7971.. \n",
      "Epoch: 64/100..  Training Loss: 0.5414..  valid Loss: 0.7443.. \n",
      "Epoch: 65/100..  Training Loss: 0.5696..  valid Loss: 0.6979.. \n",
      "Epoch: 67/100..  Training Loss: 0.5730..  valid Loss: 0.7273.. \n",
      "Epoch: 68/100..  Training Loss: 0.5859..  valid Loss: 0.7264.. \n",
      "Epoch: 69/100..  Training Loss: 0.5622..  valid Loss: 0.7294.. \n",
      "Epoch: 70/100..  Training Loss: 0.5664..  valid Loss: 0.7684.. \n",
      "Epoch: 72/100..  Training Loss: 0.5679..  valid Loss: 0.7897.. \n",
      "Epoch: 73/100..  Training Loss: 0.5568..  valid Loss: 0.7392.. \n",
      "Epoch: 74/100..  Training Loss: 0.5732..  valid Loss: 0.7370.. \n",
      "Epoch: 75/100..  Training Loss: 0.5580..  valid Loss: 0.7950.. \n",
      "Epoch: 77/100..  Training Loss: 0.5478..  valid Loss: 0.7802.. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-10cc40fb14da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#     scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PT/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PT/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PT/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PT/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PT/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PT/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SAVE_PATH = \"Models/3a_Classify/\"\n",
    "\n",
    "# writer = SummaryWriter()\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-4)\n",
    "if checkpoint_run:\n",
    "    optimizer.load_state_dict(inf['optimizer_state_dict'])\n",
    "\n",
    "best_train_loss = 0.7\n",
    "best_val_loss = 0.7\n",
    "\n",
    "# scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[5, 15, 25, 40, 50], gamma=0.1)\n",
    "\n",
    "num_epochs = 100\n",
    "running_loss = 0\n",
    "steps = 0\n",
    "print_every = 10\n",
    "log_every = 10\n",
    "log_step = 0\n",
    "\n",
    "\n",
    "log_every = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "#     scheduler.step()\n",
    "    for data_ in trainloader:\n",
    "        steps += 1\n",
    "        img, bbox = data_\n",
    "        \n",
    "        img = img.cuda()\n",
    "        target = bbox.type(torch.LongTensor).cuda()\n",
    "        \n",
    "        output = model(img)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % log_every == 0:\n",
    "            log_step += 1\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_loss = validation(model, testloader, criterion)\n",
    "            model.train()\n",
    "            \n",
    "            train_loss = running_loss/log_every\n",
    "            val_loss = valid_loss/len(testloader)\n",
    "            \n",
    "#             writer.add_scalar('Training Loss', train_loss, log_step)\n",
    "#             writer.add_scalar('Validation Loss', val_loss, log_step)\n",
    "#             writer.add_scalar('Learning rate', optimizer.state_dict()['param_groups'][0]['lr'], log_step)\n",
    "            \n",
    "            if val_loss < best_val_loss and train_loss < best_train_loss:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss\n",
    "            }, SAVE_PATH + \"res152_3a_epoch{}_step{}.pkl\".format(epoch, steps))\n",
    "                best_train_loss = train_loss\n",
    "                best_val_loss = val_loss\n",
    "            running_loss = 0\n",
    "            \n",
    "        if steps % print_every == 0:\n",
    "            print(\"Epoch: {}/{}.. \".format(epoch+1, num_epochs),\n",
    "                  \"Training Loss: {:.4f}.. \".format(train_loss),\n",
    "                  \"valid Loss: {:.4f}.. \".format(val_loss))\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = torch.load(SAVE_PATH + 'res152_3a_epoch56_step450.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5987581253051758"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model['train_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6896254420280457"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(test_model['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "y_pred = list()\n",
    "y_true = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, op in iter(testloader):\n",
    "        k = model.forward(imgs.cuda())\n",
    "        y_pred.extend(list(np.argmax(k.cpu().data.numpy(), axis=1)))\n",
    "        y_true.extend(op.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD8CAYAAAA2Y2wxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCUyJVkQksP8HGCjMX5XdjSsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f44aa392cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_pred=y_pred, y_true=y_true)\n",
    "_ = sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.73      0.72        73\n",
      "           1       0.74      0.58      0.65        50\n",
      "           2       0.62      0.75      0.68        48\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       171\n",
      "   macro avg       0.69      0.69      0.68       171\n",
      "weighted avg       0.70      0.69      0.69       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_true, y_pred=y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
